from bs4 import BeautifulSoup
import csv
import logging
import threading
import asyncio
from concurrent.futures import ThreadPoolExecutor, as_completed
from playwright.async_api import async_playwright, TimeoutError as PlaywrightTimeoutError
from pymongo import MongoClient
from pymongo.errors import ServerSelectionTimeoutError, ConnectionFailure
from config import mongo_port, db_name, product_id_and_url_collection
import psutil
import time
import random

# C·∫•u h√¨nh logging
logging.basicConfig(filename='crawl_log.txt', level=logging.INFO, format='%(asctime)s - %(message)s')

FAILED_URLS_FILE = "failed_urls.txt"
MAX_RETRIES = 2
TIMEOUT = 60000  # 60 gi√¢y timeout
DELAY_BETWEEN_REQUESTS = 1.5  # ƒê·ªô tr·ªÖ gi·ªØa c√°c y√™u c·∫ßu
MONGO_CONNECTION_RETRIES = 3
MONGO_RETRY_DELAY = 2  # Gi√¢y ch·ªù gi·ªØa c√°c l·∫ßn th·ª≠ k·∫øt n·ªëi MongoDB
MAX_THREADS = 7  # S·ªë lu·ªìng t·ªëi ƒëa
MEMORY_THRESHOLD = 90  # Ng∆∞·ª°ng b·ªô nh·ªõ (%)

# Danh s√°ch user-agent ƒë·ªÉ xoay v√≤ng
USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:125.0) Gecko/20100101 Firefox/125.0",
]

# Kh√≥a ƒë·ªÉ ƒë·ªìng b·ªô ghi file
csv_lock = threading.Lock()
failed_lock = threading.Lock()

def connect_to_mongo(mongo_port, db_name, collection_name):
    for attempt in range(1, MONGO_CONNECTION_RETRIES + 1):
        try:
            client = MongoClient(
                mongo_port,
                serverSelectionTimeoutMS=30000,
                connectTimeoutMS=30000,
                socketTimeoutMS=30000
            )
            client.admin.command('ping')
            db = client[db_name]
            collection = db[collection_name]
            logging.info("‚úÖ K·∫øt n·ªëi th√†nh c√¥ng t·ªõi MongoDB")
            return collection
        except (ServerSelectionTimeoutError, ConnectionFailure) as e:
            logging.error(f"‚ùå L·ªói k·∫øt n·ªëi MongoDB (l·∫ßn th·ª≠ {attempt}/{MONGO_CONNECTION_RETRIES}): {str(e)}")
            if attempt == MONGO_CONNECTION_RETRIES:
                raise Exception("‚ùå Kh√¥ng th·ªÉ k·∫øt n·ªëi t·ªõi MongoDB sau nhi·ªÅu l·∫ßn th·ª≠")
            time.sleep(MONGO_RETRY_DELAY)
        except Exception as e:
            logging.error(f"‚ùå L·ªói MongoDB kh√¥ng x√°c ƒë·ªãnh: {str(e)}")
            raise

async def get_product_name_async(url, retries=MAX_RETRIES):
    for attempt in range(1, retries + 1):
        browser = None
        page = None
        try:
            # Ki·ªÉm tra b·ªô nh·ªõ
            memory_percent = psutil.virtual_memory().percent
            if memory_percent > MEMORY_THRESHOLD:
                logging.error(f"‚ùå B·ªô nh·ªõ qu√° cao ({memory_percent}%) t·∫°i URL: {url}. D·ª´ng t√°c v·ª•.")
                return None

            async with async_playwright() as p:
                browser = await p.chromium.launch(headless=True)
                page = await browser.new_page(user_agent=random.choice(USER_AGENTS))
                logging.info(f"üåê Th·ª≠ {attempt}/{retries} cho URL: {url} (B·ªô nh·ªõ: {memory_percent}% ƒë√£ d√πng)")
                await page.goto(url, timeout=TIMEOUT, wait_until="domcontentloaded")
                content = await page.content()
                soup = BeautifulSoup(content, "html.parser")

                # T√¨m t√™n s·∫£n ph·∫©m, ∆∞u ti√™n span.base
                span_title = soup.find("span", class_="base", attrs={"data-ui-id": "page-title-wrapper"})
                if span_title and span_title.text.strip():
                    return span_title.text.strip()

                h1 = soup.find("h1")
                if h1 and h1.text.strip():
                    return h1.text.strip()

                title = soup.find("title")
                if title and title.text.strip():
                    return title.text.strip()

                meta = soup.find("meta", attrs={"name": "title"})
                if meta and meta.get("content"):
                    return meta["content"].strip()

                logging.info(f"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y t√™n s·∫£n ph·∫©m cho {url}")
                return None
        except PlaywrightTimeoutError as e:
            logging.error(f"‚ùå L·ªói timeout (th·ª≠ {attempt}/{retries}) cho {url}: {e}")
            if attempt == retries:
                logging.error(f"‚ùå ƒê·∫°t t·ªëi ƒëa s·ªë l·∫ßn th·ª≠ cho {url}")
                return None
            await asyncio.sleep(attempt * 2)
        except Exception as e:
            logging.error(f"‚ùå L·ªói (th·ª≠ {attempt}/{retries}) cho {url}: {e}")
            if attempt == retries:
                logging.error(f"‚ùå ƒê·∫°t t·ªëi ƒëa s·ªë l·∫ßn th·ª≠ cho {url}")
                return None
            await asyncio.sleep(attempt * 2)
        finally:
            if page:
                await page.close()
            if browser:
                await browser.close()
            await asyncio.sleep(DELAY_BETWEEN_REQUESTS)

def get_product_name(url):
    # Ch·∫°y t√°c v·ª• async trong lu·ªìng
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)
    try:
        result = loop.run_until_complete(get_product_name_async(url))
        return result
    finally:
        loop.close()

def crawl_urls(docs):
    results = []
    with ThreadPoolExecutor(max_workers=MAX_THREADS) as executor:
        future_to_url = {}
        for doc in docs:
            product_id = doc.get("product_id")
            urls = doc.get("urls", [])
            for url in urls:
                future = executor.submit(get_product_name, url)
                future_to_url[future] = (product_id, url)

        for future in as_completed(future_to_url):
            product_id, url = future_to_url[future]
            try:
                result = future.result()
                results.append((product_id, url, result))
            except Exception as e:
                logging.error(f"‚ùå L·ªói lu·ªìng cho {url}: {e}")
                results.append((product_id, url, None))
    return results

def main():
    try:
        collection = connect_to_mongo(mongo_port, db_name, product_id_and_url_collection)
        logging.info(f"üì¶ T·ªïng s·ªë t√†i li·ªáu trong collection: {collection.count_documents({})}")

        # L·∫•y t·ªëi ƒëa 10 t√†i li·ªáu, ch·ªâ l·∫•y tr∆∞·ªùng c·∫ßn thi·∫øt
        docs = list(collection.find({}, {"product_id": 1, "urls": 1}).limit(1000))

        # Crawl c√°c URL
        results = crawl_urls(docs)

        # Ghi k·∫øt qu·∫£
        with open("product_names.csv", mode="w", newline="", encoding="utf-8") as csvfile, open(FAILED_URLS_FILE, "w", encoding="utf-8") as failed_file:
            writer = csv.writer(csvfile)
            writer.writerow(["product_id", "product_name"])

            for product_id, url, result in results:
                if isinstance(result, str) and result:
                    with csv_lock:
                        writer.writerow([product_id, result])
                    logging.info(f"‚úÖ T√¨m th·∫•y: {result} cho product_id: {product_id}")
                else:
                    with failed_lock:
                        failed_file.write(f"{product_id},{url}\n")
                    logging.info(f"‚ùå Kh√¥ng t√¨m th·∫•y t√™n s·∫£n ph·∫©m cho URL: {url}")

        logging.info("üéâ HO√ÄN T·∫§T ghi t√™n s·∫£n ph·∫©m v√†o file.")
    except Exception as e:
        logging.error(f"‚ùå L·ªói nghi√™m tr·ªçng trong main: {str(e)}")
        raise

if __name__ == "__main__":
    main()